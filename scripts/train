#!/usr/bin/env python

import logging
import os
import sys
from argparse import ArgumentParser

import torch
from torch.nn.functional import binary_cross_entropy, cross_entropy
from torch.utils.data import DataLoader

from torchvision import datasets, transforms

from mnist_vae.module import VAE, DigitClassifier


_log = logging.getLogger(__name__)


arg_parser = ArgumentParser()
arg_parser.add_argument("minst_path")

num_train_samples = 60000
num_test_samples = 10000
image_size = 28


if __name__ == "__main__" :

    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)

    args = arg_parser.parse_args()

    bottleneck_dim = 20
    vae = VAE(bottleneck_dim, 1)
    classifier = DigitClassifier(bottleneck_dim)

    optimizer = torch.optim.Adam(vae.parameters())

    num_epochs = 10
    epoch_counter = 0
    while epoch_counter < num_epochs:

        dataset = datasets.MNIST(root=args.minst_path, train=True, download=False, transform=transforms.ToTensor())
        for images, true_labels in DataLoader(dataset, batch_size=64, num_workers=4, shuffle=True):

            images.to(dtype=torch.float)

            reproduced_images, bottleneck_z, predicted_mean, predicted_logvar = vae(images)

            predicted_labels_scores = classifier(bottleneck_z)

            images_loss = binary_cross_entropy(reproduced_images, images, reduction='sum')
            labels_loss = cross_entropy(predicted_labels_scores, true_labels, reduction='sum')

            Kld = -0.5 * torch.sum(1.0 + predicted_logvar - torch.square(predicted_mean) - predicted_logvar.exp())

            total_loss = images_loss + labels_loss + Kld

            _log.debug(f"total loss: {total_loss}")

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        torch.save(vae.state_dict(), "vae.pth")
        torch.save(classifier.state_dict(), "classifier.pth")

        epoch_counter += 1
